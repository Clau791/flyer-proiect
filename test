from __future__ import annotations

import logging
import os
import threading
import time
from pathlib import Path
from typing import List, Tuple, Dict
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

import ffmpeg
import pysubs2
import whisper
from deep_translator import GoogleTranslator

from services.progress_bar import send_task_progress
from services.category_v.summary_service import SummaryService


class SubtitleGenerator:
    """
    Flux complet de subtitrare:
    1) Transcriere video cu Whisper (auto detecteazÄƒ limba).
    2) Traducere via Local High-End Server (RTX 5090) cu fallback la Cloud.
    3) Scriere SRT Ã®n 'processed/'.
    4) DacÄƒ attach_mode == 'hard', arde subtitrarea Ã®n video cu ffmpeg.
    """

    def __init__(
        self,
        processed_dir: str = "processed",
        whisper_model: str = "small",
    ):
        self.processed_dir = Path(processed_dir)
        self.processed_dir.mkdir(parents=True, exist_ok=True)
        self.whisper_model_name = whisper_model
        self._model = None
        self._global_start = None
        self._total_expected = None
        self._summary_service = SummaryService()
        
        # Configurare Server Local Traducere
        # NOTÄ‚: DacÄƒ rulezi Ã®n Docker, foloseÈ™te 'http://host.docker.internal:8001/translate'
        self._local_server_url = os.getenv("LOCAL_TRANSLATION_URL", "http://localhost:8001/translate")
        
        # Configurare Sesiune HTTP cu Retry (pentru robusteÈ›e)
        self._session = requests.Session()
        retries = Retry(
            total=3,                # ÃŽncearcÄƒ de 3 ori
            backoff_factor=0.2,     # AÈ™teaptÄƒ puÈ›in Ã®ntre Ã®ncercÄƒri
            status_forcelist=[500, 502, 503, 504],
            allowed_methods=["POST"]
        )
        adapter = HTTPAdapter(max_retries=retries)
        self._session.mount("http://", adapter)
        self._session.mount("https://", adapter)

        # Configurare Legacy (Ollama / Fallback)
        self._ollama_host = os.getenv("OLLAMA_HOST", "http://127.0.0.1:11434")
        self._ollama_models = [
            os.getenv("OLLAMA_MODEL_PRIMARY", "llama3:70b"),
            os.getenv("OLLAMA_MODEL_FALLBACK", "qwen:14b"),
        ]

        # Mapare simplÄƒ pentru coduri Whisper -> Nume complete (preferate de ALMA/Qwen)
        self._lang_map = {
            "ro": "Romanian", "en": "English", "fr": "French", 
            "de": "German", "it": "Italian", "es": "Spanish",
            "ru": "Russian", "zh": "Chinese", "ja": "Japanese",
            "pt": "Portuguese"
        }
        
        # Flag intern pentru a loga succesul conexiunii o singurÄƒ datÄƒ
        self._server_connected_once = False

    # ------------ Internals ------------
    def _load_model(self):
        if self._model is None:
            # Lazy loading pentru a evita costul la import
            self._model = whisper.load_model(self.whisper_model_name)
        return self._model

    def _map_lang_code(self, code: str) -> str:
        """ConverteÈ™te 'en' -> 'English' pentru prompt-ul modelului LLM."""
        if not code: return "English"
        return self._lang_map.get(code.lower(), code.title())

    def _transcribe(self, filepath: str):
        model = self._load_model()
        # verbose=False reduce zgomotul Ã®n consolÄƒ
        result = model.transcribe(filepath, task="transcribe", verbose=False)
        segments = result.get("segments", [])
        language = result.get("language", "auto")
        return segments, language

    def _translate_via_server(self, text: str, source_code: str, target_code: str) -> str:
        """
        Trimite cerere cÄƒtre serverul local Python (FastAPI) de pe portul 8001.
        """
        source_lang = self._map_lang_code(source_code)
        target_lang = self._map_lang_code(target_code)
        
        payload = {
            "text": text,
            "source_lang": source_lang,
            "target_lang": target_lang,
            "temperature": 0.1 # TemperaturÄƒ micÄƒ pentru fidelitate maximÄƒ
        }

        try:
            # Timeout crescut la 60s pentru modele mari care pot avea latenÈ›Äƒ la primul request
            resp = self._session.post(self._local_server_url, json=payload, timeout=60)
            
            if resp.status_code == 200:
                data = resp.json()
                return data.get("translation", "").strip()
            else:
                logging.warning(f"[SUBTITLE] Server Error {resp.status_code}: {resp.text}")
                return ""
                
        except requests.exceptions.ConnectionError:
            # Aceasta este eroarea criticÄƒ dacÄƒ serverul nu merge
            logging.error(f"[SUBTITLE] ðŸ›‘ EROARE CONEXIUNE: Nu pot accesa {self._local_server_url}")
            return "CONNECTION_ERROR"
        except Exception as e:
            logging.warning(f"[SUBTITLE] Eroare generalÄƒ traducere server: {e}")
            return ""

    def _translate_ollama(self, text: str, target_lang: str) -> str:
        """Fallback method pentru Ollama standard."""
        try:
            system_prompt = (
                f"EÈ™ti un traducÄƒtor profesionist. Tradu textul EXACT Ã®n limba {target_lang}. "
                f"PÄƒstreazÄƒ sensul, numele proprii È™i ordinea logicÄƒ. "
                f"Output-ul trebuie sÄƒ conÈ›inÄƒ DOAR traducerea."
            )
            user_prompt = f"Text: {text}\nTraducere:"
            
            for mdl in self._ollama_models:
                try:
                    resp = requests.post(
                        f"{self._ollama_host}/v1/completions",
                        json={
                            "model": mdl,
                            "prompt": f"{system_prompt}\n\n{user_prompt}",
                            "options": {"temperature": 0.1},
                        },
                        timeout=60,
                    )
                    if resp.ok:
                        data = resp.json()
                        txt = (data.get("choices") or [{}])[0].get("text", "").strip()
                        if txt: return txt
                except:
                    continue
        except Exception as e:
            logging.warning(f"[SUBTITLE] Ollama fallback failed: {e}")
        return ""

    def _translate_segments(
        self, 
        segments: List[dict], 
        source_lang_detected: str, 
        target_lang: str, 
        mode: str = "local_server"
    ) -> Tuple[List[str], List[str]]:
        """
        ReturneazÄƒ (texte_traduse, texte_originale).
        """
        translated, originals = [], []
        
        # IniÈ›ializÄƒm Cloud Translator doar dacÄƒ e cerut explicit sau ca fallback
        cloud_translator = None
        
        server_available = True
        
        # ResetÄƒm flag-ul de conexiune pentru acest job
        self._server_connected_once = False

        for i, seg in enumerate(segments):
            orig = (seg.get("text") or "").strip()
            originals.append(orig)
            
            if not orig:
                translated.append("")
                continue

            # Logica de selecÈ›ie a metodei
            translation = ""

            # 1. ÃŽncercÄƒm Serverul Local High-End (Prioritate)
            if mode == "local_server" and server_available:
                translation = self._translate_via_server(orig, source_lang_detected, target_lang)
                
                if translation == "CONNECTION_ERROR":
                    logging.warning("[SUBTITLE] âš ï¸ Serverul local nu a rÄƒspuns. ComutÄƒm automat pe Google Translate.")
                    server_available = False # Circuit breaker: oprim Ã®ncercÄƒrile spre server
                    translation = "" 
                elif translation:
                    # LogÄƒm succesul o singurÄƒ datÄƒ per video, ca sÄƒ È™tim cÄƒ merge
                    if not self._server_connected_once:
                        logging.info(f"[SUBTITLE] âœ… Conexiune stabilÄƒ cu Serverul Local ({self._local_server_url}). Traducere neuralÄƒ activÄƒ.")
                        self._server_connected_once = True
                
                # DacÄƒ translation e gol dar nu e connection error, serverul a returnat gol -> Ã®ncercÄƒm fallback

            # 2. Fallback: Google Translate (dacÄƒ serverul a picat sau mode="cloud")
            if not translation:
                try:
                    if not cloud_translator:
                        # IniÈ›ializare lazy
                        cloud_translator = GoogleTranslator(source="auto", target=target_lang)
                    translation = cloud_translator.translate(orig)
                except Exception as e:
                    # Log doar la debug ca sÄƒ nu umplem consola
                    logging.debug(f"Google translate fail: {e}")

            # 3. Fallback Ultim: Ollama vechi (dacÄƒ existÄƒ È™i Google a eÈ™uat)
            if not translation:
                translation = self._translate_ollama(orig, target_lang)

            # 4. Fail-safe: Originalul
            translated.append(translation if translation else orig)

            # Log progress discret (la fiecare 10 segmente)
            if i % 10 == 0:
                logging.info(f"[SUBTITLE] Tradus segment {i}/{len(segments)}")

        return translated, originals

    def _write_srt(self, segments: List[dict], texts: List[str], srt_path: Path):
        subs = pysubs2.SSAFile()
        for seg, txt in zip(segments, texts):
            start_ms = int(float(seg.get("start", 0)) * 1000)
            end_ms = int(float(seg.get("end", 0)) * 1000)
            event = pysubs2.SSAEvent(start=start_ms, end=end_ms, text=txt)
            subs.append(event)
        subs.save(str(srt_path), encoding="utf-8")

    def _burn_subtitles(self, video_path: Path, srt_path: Path, output_path: Path):
        srt_escaped = srt_path.as_posix().replace("'", r"\\'")
        (
            ffmpeg
            .input(str(video_path))
            .output(
                str(output_path),
                vf=f"subtitles='{srt_escaped}'",
                acodec="copy",
                vcodec="libx264",
                preset="fast",
                crf=18,
            )
            .overwrite_output()
            .run(quiet=True)
        )

    def _probe_duration(self, video_path: Path) -> float:
        try:
            meta = ffmpeg.probe(str(video_path))
            for stream in meta.get("streams", []):
                if stream.get("codec_type") == "video" and stream.get("duration"):
                    return float(stream["duration"])
            if "format" in meta and meta["format"].get("duration"):
                return float(meta["format"]["duration"])
        except Exception:
            return 0.0
        return 0.0

    def _estimate_timeline(self, duration: float, attach_mode: str):
        rt_factor = 1.2
        translate_factor = 0.3 # PuÈ›in mai mult pentru inferenÈ›Äƒ localÄƒ de calitate
        burn_factor = 0.5
        summary_factor = 0.15

        duration = max(duration, 60.0)
        est_transcribe = duration * rt_factor
        est_translate = duration * translate_factor
        est_burn = duration * burn_factor if attach_mode == "hard" else 0.0
        est_summary = duration * summary_factor
        
        return {
            "transcribe": est_transcribe,
            "translate": est_translate,
            "burn": est_burn,
            "summary": est_summary,
            "total": est_transcribe + est_translate + est_burn + est_summary + 10.0,
        }

    def _run_stage(self, label: str, expected_seconds: float, base_percent: float, weight_percent: float, func):
        start = time.time()
        done = threading.Event()

        def ticker():
            while not done.is_set():
                elapsed = time.time() - start
                ratio = min(1.0, elapsed / max(expected_seconds, 0.1))
                percent = base_percent + ratio * weight_percent
                eta = max(0.0, (self._total_expected or expected_seconds) - (time.time() - self._global_start))
                send_task_progress(percent=percent, eta_seconds=eta, stage=label, detail="Ã®n curs")
                time.sleep(1.0)

        t = threading.Thread(target=ticker, daemon=True)
        t.start()
        try:
            result = func()
        finally:
            done.set()
            t.join(timeout=0.2)

        percent = base_percent + weight_percent
        eta = max(0.0, (self._total_expected or expected_seconds) - (time.time() - self._global_start))
        send_task_progress(percent=percent, eta_seconds=eta, stage=label, detail="finalizat")
        return result

    # ------------ API publicÄƒ ------------
    def generate(self, filepath: str, lang: str = "ro", attach_mode: str = "soft", detail_level: str = "medium", translator_mode: str = "local_server"):
        """
        GenereazÄƒ subtitrare.
        
        Args:
            translator_mode: 'local_server' (Recomandat - RTX 5090), 'cloud' (Google), 'ollama'.
        """
        video_path = Path(filepath)
        attach_mode = (attach_mode or "soft").lower()
        translator_mode = (translator_mode or "local_server").lower()
        
        print(f"[SUBTITLE] Start: Mode={attach_mode}, Translator={translator_mode}")
        self._global_start = time.time()

        duration = self._probe_duration(video_path)
        timeline = self._estimate_timeline(duration, attach_mode)
        self._total_expected = timeline["total"]

        # Ponderi progres
        pct_transcribe = 50.0
        pct_translate = 30.0 # CreÈ™tem ponderea traducerii fiind neuralÄƒ
        pct_summary = 10.0
        pct_burn = 0.0 if attach_mode != "hard" else 10.0
        
        send_task_progress(percent=1.0, eta_seconds=timeline["total"], stage="init", detail="pregÄƒtire")

        # 1. Transcriere
        segments, detected_lang = self._run_stage(
            "transcriere",
            timeline["transcribe"], 0.0, pct_transcribe,
            lambda: self._transcribe(str(video_path)),
        )
        print(f"[SUBTITLE] LimbÄƒ detectatÄƒ: {detected_lang}")

        # 2. Traducere (Aici e magia nouÄƒ)
        print(f"[SUBTITLE] ÃŽncepe traducerea cu {translator_mode}...")
        translated_texts, originals = self._run_stage(
            "traducere",
            timeline["translate"], pct_transcribe, pct_translate,
            lambda: self._translate_segments(
                segments, 
                source_lang_detected=detected_lang, 
                target_lang=lang, 
                mode=translator_mode
            ),
        )

        # 3. Scriere SRT
        srt_path = self.processed_dir / f"{video_path.stem}_{lang}.srt"
        self._write_srt(segments, translated_texts, srt_path)

        # 4. Rezumat
        full_text = "\n".join(translated_texts)
        summary_result = self._run_stage(
            "rezumat",
            timeline["summary"], pct_transcribe + pct_translate, pct_summary,
            lambda: self._summary_service.summarize_content(
                content_id=video_path.stem,
                text=full_text,
                metadata={"source": "video", "lang": lang, "detail": detail_level},
            ),
        )

        response = {
            "attach_mode": attach_mode,
            "subtitle_file": f"/download/{srt_path.name}",
            "subtitle_language": lang,
            "detected_language": detected_lang,
            "segments": len(segments),
            "summary": summary_result.get("summary"),
        }

        # 5. Hard Burn (OpÈ›ional)
        if attach_mode == "hard":
            output_video = self.processed_dir / f"{video_path.stem}_{lang}_subtitled.mp4"
            self._run_stage(
                "burn",
                timeline["burn"], pct_transcribe + pct_translate + pct_summary, pct_burn,
                lambda: self._burn_subtitles(video_path, srt_path, output_video),
            )
            response["video_file"] = f"/download/{output_video.name}"
        else:
            response["video_file"] = f"/download/{srt_path.name}"

        send_task_progress(percent=100.0, eta_seconds=0.0, stage="gata", detail="complet")
        return response
