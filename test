import torch
import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# --- CONFIGURARE MODEL ---
# X-ALMA-13B-Group2: Varianta specializatÄƒ pe Limbi Romanice (inclusiv ROMÃ‚NÄ‚)
# Aceasta este versiunea "Merged", gata de utilizare directÄƒ.
MODEL_ID = "haoranxu/X-ALMA-13B-Group2"

# SetÄƒri Server
HOST = "0.0.0.0"
PORT = 8000

app = FastAPI(title="X-ALMA Translation API (Romanian Specialist)", version="4.0")

model = None
tokenizer = None

class TranslationRequest(BaseModel):
    text: str
    source_lang: str = "English"
    target_lang: str = "Romanian"
    # X-ALMA nu are nevoie de temperaturÄƒ mare, este foarte determinist
    temperature: float = 0.1 

@app.on_event("startup")
async def load_model():
    global model, tokenizer
    print(f"ğŸš€ Se iniÈ›ializeazÄƒ Specialistul: {MODEL_ID}...")
    print("â„¹ï¸ Acest model face parte din familia X-ALMA, optimizatÄƒ specific pentru limbi romanice.")
    print("ğŸ“‰ Optimizare: Se foloseÈ™te 8-bit Quantization (Calitate SuperioarÄƒ).")

    try:
        # 1. Configurare Cuantizare 8-bit
        # Pe RTX 5090 (32GB), un model de 13B Ã®n 8-bit ocupÄƒ doar ~14GB.
        # Alegem 8-bit Ã®n loc de 4-bit pentru a pÄƒstra fiecare nuanÈ›Äƒ a traducerii.
        bnb_config = BitsAndBytesConfig(
            load_in_8bit=True,
        )

        # 2. ÃncÄƒrcare Tokenizer
        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
        
        # 3. ÃncÄƒrcare Model
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_ID,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )
        
        print(f"âœ… X-ALMA Group2 ÃncÄƒrcat! EÈ™ti gata pentru traduceri de precizie Ã®n RomÃ¢nÄƒ.")
        
    except Exception as e:
        print(f"âŒ Eroare la Ã®ncÄƒrcare: {e}")
        raise e

@app.post("/translate")
async def translate_text(request: TranslationRequest):
    if not model or not tokenizer:
        raise HTTPException(status_code=503, detail="Modelul se Ã®ncarcÄƒ...")

    # X-ALMA foloseÈ™te un prompt simplu de chat pentru a activa traducerea
    # Formatul recomandat de autori:
    prompt_content = f"Translate this from {request.source_lang} to {request.target_lang}:\n{request.text}"
    
    messages = [
        {"role": "user", "content": prompt_content}
    ]

    # AplicÄƒm template-ul de chat specific X-ALMA
    text_input = tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    inputs = tokenizer([text_input], return_tensors="pt").to("cuda")

    try:
        # InferenÈ›Äƒ X-ALMA
        # Folosim Beam Search modest (num_beams=3) pentru a garanta calitatea gramaticalÄƒ
        # fÄƒrÄƒ a Ã®ncetini excesiv procesul.
        generated_ids = model.generate(
            inputs.input_ids,
            max_new_tokens=4096, # SuportÄƒ texte lungi
            num_beams=3,         # Balans ideal Ã®ntre vitezÄƒ È™i corectitudine gramaticalÄƒ
            do_sample=False,     # Determinism pur pentru traduceri tehnice/oficiale
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id
        )

        # Extragem doar rÄƒspunsul
        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)
        ]
        response_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

        return {
            "source": request.source_lang,
            "target": request.target_lang,
            "model": "X-ALMA-13B-Group2",
            "translation": response_text.strip()
        }

    except torch.cuda.OutOfMemoryError:
        torch.cuda.empty_cache()
        raise HTTPException(status_code=500, detail="VRAM Insuficient. ÃncearcÄƒ un text mai scurt.")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run("translation_server:app", host=HOST, port=PORT, reload=False)
