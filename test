import re
import torch
import uvicorn
import logging
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, validator
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# --- CONFIGURARE LOGGING ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# --- CONFIGURARE MODEL ---
# QwQ-32B-Preview: Model de reasoning foarte puternic
MODEL_ID = "Qwen/QwQ-32B-Preview"
HOST = "0.0.0.0"
PORT = 8002  # Port diferit faÈ›Äƒ de ALMA (8001)

app = FastAPI(
    title="QwQ-32B Translation API (Reasoning Specialist)",
    version="1.0",
    description="API de traducere strictÄƒ folosind QwQ-32B cu suprimarea reasoning-ului"
)

model = None
tokenizer = None

class TranslationRequest(BaseModel):
    text: str
    source_lang: str = "English"
    target_lang: str = "Romanian"
    max_length: int = 4096
    
    @validator('text')
    def validate_text(cls, v):
        if not v or len(v.strip()) == 0:
            raise ValueError("Textul nu poate fi gol")
        return v.strip()

def clean_qwq_output(text: str) -> str:
    """
    CurÄƒÈ›Äƒ outputul specific modelelor de reasoning (QwQ/DeepSeek).
    EliminÄƒ tag-urile <think> È™i explicaÈ›iile.
    """
    if not text:
        return ""

    # 1. EliminÄƒ blocuri de gÃ¢ndire <think>...</think> (inclusiv pe mai multe linii)
    cleaned = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    
    # 2. EliminÄƒ doar tag-urile rÄƒmase (dacÄƒ modelul a uitat sÄƒ le Ã®nchidÄƒ)
    cleaned = re.sub(r'<think>', '', cleaned)
    cleaned = re.sub(r'</think>', '', cleaned)

    # 3. EliminÄƒ prefixe comune de chat
    patterns = [
        r'^(?:Here is the translation|Translation|Traducere|RÄƒspuns):',
        r'^Sure, here is.*?:',
        r'^Certainly.*?:',
    ]
    for p in patterns:
        cleaned = re.sub(p, '', cleaned, flags=re.IGNORECASE)

    # 4. EliminÄƒ ghilimele exterioare
    cleaned = cleaned.strip()
    if cleaned.startswith('"') and cleaned.endswith('"'):
        cleaned = cleaned[1:-1]
    
    return cleaned.strip()

@app.on_event("startup")
async def load_model():
    global model, tokenizer
    logger.info(f"ğŸš€ Se iniÈ›ializeazÄƒ QwQ-32B: {MODEL_ID}...")
    logger.info("â„¹ï¸ Optimizare: 4-bit NF4 (Mandatoriu pentru 32B pe 32GB VRAM)")
    
    try:
        # Configurare Cuantizare 4-bit (NF4)
        # 32B params * 0.5 bytes = ~16GB VRAM modelul + context overhead
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True
        )

        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
        
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_ID,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )
        
        logger.info(f"âœ… QwQ-32B ÃncÄƒrcat pe Portul {PORT}!")
        
    except Exception as e:
        logger.error(f"âŒ Eroare la Ã®ncÄƒrcare QwQ: {e}", exc_info=True)
        raise e

@app.post("/translate")
async def translate_text(request: TranslationRequest):
    if not model or not tokenizer:
        raise HTTPException(status_code=503, detail="Modelul se Ã®ncarcÄƒ...")

    # --- PROMPT STRICT PENTRU QwQ ---
    # QwQ tinde sÄƒ gÃ¢ndeascÄƒ mult. Ãl forÈ›Äƒm sÄƒ fie direct.
    system_instruction = (
        "You are a strict translation engine. Your ONLY task is to translate the input text. "
        "Rules:\n"
        "1. Output ONLY the translation.\n"
        "2. Do NOT output reasoning, thinking process, or <think> tags.\n"
        "3. Do NOT provide explanations, notes, or introductions.\n"
        "4. Maintain the tone and style of the original text."
    )
    
    user_content = f"Translate from {request.source_lang} to {request.target_lang}:\n{request.text}"

    messages = [
        {"role": "system", "content": system_instruction},
        {"role": "user", "content": user_content}
    ]
    
    # QwQ foloseÈ™te template-ul standard ChatML sau Qwen
    text_input = tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    inputs = tokenizer([text_input], return_tensors="pt").to("cuda")

    try:
        with torch.no_grad():
            generated_ids = model.generate(
                inputs.input_ids,
                max_new_tokens=request.max_length,
                do_sample=False,       # Greedy decoding pentru stricteÈ›e maximÄƒ
                temperature=None,      # Ignorat la greedy, dar setat explicit
                top_p=None,            # Ignorat la greedy
                repetition_penalty=1.1 # Previne buclele Ã®n traduceri lungi
            )

        # Extrage doar rÄƒspunsul nou
        generated_ids = [
            output_ids[len(input_ids):] 
            for input_ids, output_ids in zip(inputs.input_ids, generated_ids)
        ]
        
        raw_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        
        # CurÄƒÈ›are agresivÄƒ
        cleaned_translation = clean_qwq_output(raw_output)
        
        return {
            "source": request.source_lang,
            "target": request.target_lang,
            "model": "Qwen/QwQ-32B-Preview",
            "translation": cleaned_translation
        }

    except torch.cuda.OutOfMemoryError:
        logger.error("âŒ VRAM Insuficient pe QwQ!")
        torch.cuda.empty_cache()
        raise HTTPException(status_code=507, detail="VRAM Insuficient. Textul e prea lung pentru contextul rÄƒmas.")
    except Exception as e:
        logger.error(f"Eroare inferenÈ›Äƒ: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run("qwq_server:app", host=HOST, port=PORT, reload=False)
