import re
import torch
import uvicorn
import logging
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, validator
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# --- CONFIGURARE LOGGING ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# --- CONFIGURARE MODEL ---
MODEL_ID = "haoranxu/X-ALMA-13B-Group2"
HOST = "0.0.0.0"
PORT = 8001

app = FastAPI(
    title="X-ALMA Translation API (Romanian Specialist)",
    version="5.0",
    description="API de traducere optimizat pentru rom√¢nƒÉ folosind X-ALMA-13B-Group2"
)

model = None
tokenizer = None

# --- PATTERN-URI PENTRU CURƒÇ»öARE ---
CLEANUP_PATTERNS = [
    # Ghilimele »ôi apostrof
    (r'^["\'](.+)["\']$', r'\1'),
    (r'^"(.+)"$', r'\1'),
    (r"^'(.+)'$", r'\1'),
    
    # Prefixe comune √Æn rƒÉspunsuri (case insensitive)
    (r'^(?:Traducere|Translation|RƒÉspuns|Raspuns|Answer|Result|Output)\s*:\s*', '', re.IGNORECASE),
    (r'^(?:√én|In)\s+(?:limba\s+)?(?:rom√¢nƒÉ|romana|romanƒÉ|english)\s*:\s*', '', re.IGNORECASE),
    (r'^(?:Here is the translation|IatƒÉ traducerea)\s*:\s*', '', re.IGNORECASE),
    
    # Markdown »ôi formatare
    (r'^\*\*(.+)\*\*$', r'\1'),
    (r'^`(.+)`$', r'\1'),
    
    # Spa»õii multiple »ôi newline-uri la √Ænceput/final
    (r'^\s+|\s+$', ''),
    (r'\n{3,}', '\n\n'),
]

def clean_translation_output(text: str) -> str:
    """
    CurƒÉ»õƒÉ outputul de la model elimin√¢nd:
    - Ghilimele redundante
    - Prefixe de tip "RƒÉspuns:", "In limba romana:", etc.
    - Formatare markdown inutilƒÉ
    - Spa»õii excesive
    """
    if not text:
        return text
    
    cleaned = text.strip()
    
    # AplicƒÉ toate pattern-urile de curƒÉ»õare
    for pattern_data in CLEANUP_PATTERNS:
        if len(pattern_data) == 2:
            pattern, replacement = pattern_data
            flags = 0
        else:
            pattern, replacement, flags = pattern_data
        
        cleaned = re.sub(pattern, replacement, cleaned, flags=flags)
        cleaned = cleaned.strip()
    
    # EliminƒÉ ghilimele doar la √Ænceput »ôi final (nu din mijlocul textului)
    if cleaned.startswith('"') and cleaned.endswith('"'):
        cleaned = cleaned[1:-1].strip()
    if cleaned.startswith("'") and cleaned.endswith("'"):
        cleaned = cleaned[1:-1].strip()
    
    # CurƒÉ»õƒÉ spa»õii multiple
    cleaned = re.sub(r'\s+', ' ', cleaned)
    
    # RestaureazƒÉ newline-uri normale (pƒÉstreazƒÉ paragrafele)
    cleaned = re.sub(r'\s*\n\s*', '\n', cleaned)
    
    logger.debug(f"CurƒÉ»õare: '{text[:50]}...' -> '{cleaned[:50]}...'")
    
    return cleaned

class TranslationRequest(BaseModel):
    text: str
    source_lang: str = "English"
    target_lang: str = "Romanian"
    max_length: int = 2048
    use_beam_search: bool = True
    num_beams: int = 3
    
    @validator('text')
    def validate_text(cls, v):
        if not v or len(v.strip()) == 0:
            raise ValueError("Textul nu poate fi gol")
        if len(v) > 15000:
            raise ValueError("Text prea lung (max 15000 caractere pentru context mare)")
        return v.strip()
    
    @validator('max_length')
    def validate_max_length(cls, v):
        if v < 128 or v > 4096:
            raise ValueError("max_length trebuie sƒÉ fie √Æntre 128 »ôi 4096")
        return v
    
    @validator('num_beams')
    def validate_num_beams(cls, v):
        if v < 1 or v > 5:
            raise ValueError("num_beams trebuie sƒÉ fie √Æntre 1 »ôi 5")
        return v

class BatchTranslationRequest(BaseModel):
    texts: list[str]
    source_lang: str = "English"
    target_lang: str = "Romanian"
    max_length: int = 2048
    
    @validator('texts')
    def validate_texts(cls, v):
        if not v or len(v) == 0:
            raise ValueError("Lista de texte nu poate fi goalƒÉ")
        if len(v) > 50:
            raise ValueError("Maximum 50 texte per batch")
        for text in v:
            if len(text) > 15000:
                raise ValueError("Unul dintre texte depƒÉ»ôe»ôte 15000 caractere")
        return v

@app.on_event("startup")
async def load_model():
    global model, tokenizer
    logger.info(f"üöÄ Se ini»õializeazƒÉ Specialistul: {MODEL_ID}...")
    logger.info("‚ÑπÔ∏è Arhitectura: Decoder-Only (LLM) - LLaMA Based")
    
    try:
        # Configurare Cuantizare 8-bit
        bnb_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
            llm_int8_has_fp16_weight=False
        )

        # √éncƒÉrcare Tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_ID,
            trust_remote_code=True,
            use_fast=True
        )
        
        # Setare padding token dacƒÉ lipse»ôte
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # √éncƒÉrcare Model
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_ID,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True
        )
        
        model.eval()  # Setare modul evaluare
        
        logger.info(f"‚úÖ X-ALMA Group2 √éncƒÉrcat! VRAM optimizatƒÉ cu 8-bit quantization.")
        logger.info(f"üìä Device map: {model.hf_device_map}")
        
    except Exception as e:
        logger.error(f"‚ùå Eroare la √ÆncƒÉrcare: {e}", exc_info=True)
        raise e

@app.get("/health")
async def health_check():
    """VerificƒÉ starea serverului »ôi modelului"""
    cuda_available = torch.cuda.is_available()
    vram_allocated = torch.cuda.memory_allocated(0) / 1024**3 if cuda_available else 0
    vram_reserved = torch.cuda.memory_reserved(0) / 1024**3 if cuda_available else 0
    
    return {
        "status": "healthy" if model is not None else "loading",
        "model": MODEL_ID,
        "model_loaded": model is not None,
        "device": "cuda" if cuda_available else "cpu",
        "vram_allocated_gb": round(vram_allocated, 2),
        "vram_reserved_gb": round(vram_reserved, 2)
    }

@app.post("/translate")
async def translate_text(request: TranslationRequest):
    """Traduce un singur text"""
    if not model or not tokenizer:
        raise HTTPException(status_code=503, detail="Modelul se √ÆncarcƒÉ...")

    logger.info(f"üìù Traducere: {request.source_lang} -> {request.target_lang}, "
                f"{len(request.text)} caractere")
    
    # Prompt simplu »ôi clar (X-ALMA preferƒÉ simplitate)
    prompt = f"Translate this from {request.source_lang} to {request.target_lang}:\n{request.text}"
    
    messages = [{"role": "user", "content": prompt}]
    
    try:
        text_input = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    except Exception:
        # Fallback dacƒÉ chat template nu func»õioneazƒÉ
        text_input = prompt
    
    inputs = tokenizer(
        [text_input],
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=4096
    ).to("cuda")

    try:
        with torch.no_grad():
            generated_ids = model.generate(
                inputs.input_ids,
                attention_mask=inputs.attention_mask,
                max_new_tokens=request.max_length,
                num_beams=request.num_beams if request.use_beam_search else 1,
                do_sample=False,
                temperature=None,
                repetition_penalty=1.15,
                length_penalty=1.0,
                early_stopping=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                no_repeat_ngram_size=3
            )

        # EliminƒÉ promptul din output
        generated_ids = [
            output_ids[len(input_ids):] 
            for input_ids, output_ids in zip(inputs.input_ids, generated_ids)
        ]
        
        response_text = tokenizer.batch_decode(
            generated_ids,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True
        )[0]
        
        # CURƒÇ»öARE OUTPUT
        cleaned_text = clean_translation_output(response_text)
        
        logger.info(f"‚úÖ Traducere completƒÉ: {len(cleaned_text)} caractere")
        
        return {
            "source": request.source_lang,
            "target": request.target_lang,
            "model": "X-ALMA-13B-Group2",
            "translation": cleaned_text,
            "raw_output": response_text,  # Pentru debugging
            "stats": {
                "chars_in": len(request.text),
                "chars_out": len(cleaned_text),
                "tokens_generated": len(generated_ids[0])
            }
        }

    except torch.cuda.OutOfMemoryError:
        logger.error("‚ùå VRAM insuficient!")
        torch.cuda.empty_cache()
        raise HTTPException(status_code=507, detail="VRAM insuficient. √éncearcƒÉ un text mai scurt.")
    except Exception as e:
        logger.error(f"‚ùå Eroare la traducere: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Eroare: {str(e)}")
    finally:
        # CurƒÉ»õare memorie dupƒÉ fiecare request
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

@app.post("/translate/batch")
async def translate_batch(request: BatchTranslationRequest):
    """Traduce multiple texte simultan (mai eficient)"""
    if not model or not tokenizer:
        raise HTTPException(status_code=503, detail="Modelul se √ÆncarcƒÉ...")

    logger.info(f"üì¶ Batch traducere: {len(request.texts)} texte, "
                f"{request.source_lang} -> {request.target_lang}")
    
    prompts = [
        f"Translate this from {request.source_lang} to {request.target_lang}:\n{text}"
        for text in request.texts
    ]
    
    try:
        # Procesare batch
        inputs = tokenizer(
            prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to("cuda")

        with torch.no_grad():
            generated_ids = model.generate(
                inputs.input_ids,
                attention_mask=inputs.attention_mask,
                max_new_tokens=request.max_length,
                num_beams=2,  # Redus pentru batch
                do_sample=False,
                repetition_penalty=1.15,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )

        # EliminƒÉ prompturile
        generated_ids = [
            output_ids[len(input_ids):]
            for input_ids, output_ids in zip(inputs.input_ids, generated_ids)
        ]
        
        response_texts = tokenizer.batch_decode(
            generated_ids,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True
        )
        
        # CurƒÉ»õare pentru fiecare output
        cleaned_texts = [clean_translation_output(text) for text in response_texts]
        
        logger.info(f"‚úÖ Batch completat: {len(cleaned_texts)} traduceri")
        
        return {
            "source": request.source_lang,
            "target": request.target_lang,
            "model": "X-ALMA-13B-Group2",
            "translations": cleaned_texts,
            "count": len(cleaned_texts)
        }

    except torch.cuda.OutOfMemoryError:
        logger.error("‚ùå VRAM insuficient pentru batch!")
        torch.cuda.empty_cache()
        raise HTTPException(status_code=507, detail="VRAM insuficient. Reduce numƒÉrul de texte.")
    except Exception as e:
        logger.error(f"‚ùå Eroare la batch: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Eroare: {str(e)}")
    finally:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

@app.post("/translate/debug")
async def translate_debug(request: TranslationRequest):
    """
    Endpoint de debugging care returneazƒÉ at√¢t outputul curat c√¢t »ôi raw,
    pentru a testa pattern-urile de curƒÉ»õare
    """
    if not model or not tokenizer:
        raise HTTPException(status_code=503, detail="Modelul se √ÆncarcƒÉ...")

    prompt = f"Translate this from {request.source_lang} to {request.target_lang}:\n{request.text}"
    messages = [{"role": "user", "content": prompt}]
    
    try:
        text_input = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    except:
        text_input = prompt
    
    inputs = tokenizer([text_input], return_tensors="pt").to("cuda")

    try:
        with torch.no_grad():
            generated_ids = model.generate(
                inputs.input_ids,
                max_new_tokens=request.max_length,
                num_beams=request.num_beams,
                do_sample=False,
                repetition_penalty=1.15,
                pad_token_id=tokenizer.pad_token_id
            )

        generated_ids = [
            output_ids[len(input_ids):] 
            for input_ids, output_ids in zip(inputs.input_ids, generated_ids)
        ]
        
        response_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        cleaned_text = clean_translation_output(response_text)
        
        return {
            "raw_output": response_text,
            "cleaned_output": cleaned_text,
            "removed_chars": len(response_text) - len(cleaned_text),
            "prompt_used": text_input
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

if __name__ == "__main__":
    uvicorn.run(
        "alma_server:app",
        host=HOST,
        port=PORT,
        reload=False,
        workers=1,
        log_level="info"
    )
